\chapter{Adaptive Compression}

\label{ch:adaptivecompression}

\section{Cracking}

In this chapter we examine ways of performing online compression of the adjacency list representing
the graph by modifying the existing cracking algorithms presented by Ideos' et al.. The background
subsection on cracking covers the terms we will use in this chapter to explain the modifications we
made.

\section{Compression}

In this section we demonstrate that online restructuring of an adjacency list into a CSR formatted 
adjacency matrix is feasible by using cracking, and discuss how this can be exploited to improve the
performance of the execution of graph traversal algorithms.

Our motivation for applying compression is to acheive a speed up by reducing the number of memory
accesses required in the execution of an algorithm. We will demonstrate in section
\ref{ss:compressionexploitation} how we exploit compression.

\subsection{Opportunities}

When applying a run-length encoding to either the $src$ or $dst$ columns, we can gather compression
information with varying eagerness in the course of a cracking scan. We could reserve a decision on
compression until we know that an entire column fragment can be compressed, or we can make many small
compressions in the course of scans of a column, which would require us to store all that
information.

Todo: Make some pictures in that program on the macs in labs

\subsubsection{Per-fragment}
\label{sss:perfragment}

A per-fragment compression policy would be to compress only when entire column fragments are
known to be homogenous. After performing a scan, the cracker index is updated with the boundary
values for any created fragment(s). If two values are stored which are \texttt{minimally different},
then we can compress the column between those indices, which correspond to the boundary values of the
homogenous fragment.

We use the phrase \texttt{minimally different} to describe two values for which there exists no value
between them in the range of values that can be taken for their common type. For example, if we
consider the integers, the values 2 and 3 are minimally different, because there exists no integer
between these values. For a datatype such as a floating point number, this value will vary depending
on the accuracy with which the user wishes to cluster - it may be acceptable to the application that
the values 2.33334 and 2.33279 both be considered 2.33, however, unlike in the integer case, there is
information being lost during compression, which may not be acceptable to the user, which would prompt
them to use a greater granularity for the compression, or to not compress at all.

Compared to the RLE method, this is very easy to implement. One must simply check the boundaries of
newly created fragments and determine if any fragments can now be known to be homogenous.

\subsubsection{Run-length encoding}

In the most eager case, we build and maintain information about all runs of consecutive nodes in the
cracker column of the adjacency list. This is done inside an array, $run\_lengths$.

The cracker column is scanned both front to back and back to front, therefore it would be preferable
that information be stored such that it can be applied in both scanning directions. For a given run of
consecutive values in the cracker column from index $i$ to index $j$ inclusive, the $run\_lengths$
array stores the value $1 + j - i$ at both indices $i$ and $j$, that is, the number of values in the
consecutive run. Therefore, when a pointer is moving from front to back, it can see the run length for
a given value, and a pointer moving in the opposite direction can see it also.

We also need to maintain the $run\_lengths$ array under the restructuring that takes place during
cracking to make it worthwhile, which requires that we be be careful in maintaining all of the
necessary invariants regarding the position of the two tightening edge pointers used during cracking.
This must also take account of the fact that the number of elements being swapped to an edge
pointer during an iteration may be different to the number of elements being swapped from the edge
pointer back onto the iteration pointer. Additionally, when making these swaps of runs, the
$run\_lengths$ array may have need to have its entries modified in order to retain consistency.


\subsection{To compact or not to compact}

After identifying a column fragment containing edges only to/from a single vertex, we have a choice 
in how we proceed in compressing it. We can simply apply the knowledge that a fragment is uniform,
without making any structural changes to the data, or we can actually compact the run-length
encoded fragment, deleting the duplicated elements.

\subsubsection{Cost of compaction}

It is important to note that the columns are implemented as arrays, and as such all reside in
contigious memory, therefore when a range of contigiuos elements are deleted, the elements after
that range must be shifted back towards the front of the array. This memory shift is expensive,
and the cost increases with the amount of data that must be copied over.

It can be said that compaction saves memory, however if it was crucial to save memory, then cracking
would not be an appropriate technique anyway, so we do not consider it an important benefit.

\subsubsection{Without compaction}

By identifying a uniform fragment, we can know that there is no need to read more than one element
of a given run for a cracked column. Therefore we do not need to send the repeated values from
memory over to the application, instead sending the single value and the corresponding rows of the
table. In that case that we have selected multiple compressed vertices, we need to store offsets
into the vector of tuples we return in order to let the application know which tuples correspond to
which compressed value.

The main advantage of this method is that it avoids the deletion overhead incurred by compcating,
which saves time during queries in which a compression opportunity first arises.

\subsection{Exploitation}
\label{ss:compressionexploitation}
