\chapter{Adaptive Compression}

\label{ch:adaptivecompression}

\section{Cracking}

We model a graph using an adjacency list and depending on the algorithm we crack on either the $src$
or $dst$ columns, for example, in pagerank we must iterate over the in-neighbours of vertices,
whereas for an algorithm like BFS we iterate over the out-neighbours.

We have used a decomposed storage format as is used in the original cracking paper.

\section{Compression}

In this section we demonstrate that online restructuring of an adjacency list into a CSR formatted 
adjacency matrix is feasible by using cracking, and discuss how this can be exploited to improve the
performance of the execution of graph traversal algorithms.

Our motivation for applying compression is to acheive a speed up by reducing the number of memory
accesses required in the execution of an algorithm. We will demonstrate in section
\ref{ss:compressionexploitation} how we exploit compression.

\subsection{Opportunities}

When applying a run-length encoding to either the $src$ or $dst$ columns, we can gather compression
information with varying eagerness in the course of a cracking scan. We could reserve a decision on
compression until we know that an entire column fragment can be compressed, or we can make
many small compressions in the course of scans of a column, which would require us to store all that
information.

\subsubsection{Run-length encoding}

In the most eager case, we build and maintain information about all runs of consecutive nodes in the
cracker column of the adjacency list. This is done inside an array, $run\_lengths$.

The cracker column is scanned both front to back and back to front, therefore it would be preferable
that information be stored such that it can be applied in both scanning directions. For a given run of
consecutive values in the cracker column from index $i$ to index $j$ inclusive, the $run\_lengths$
array stores the value $1 + j - i$ at both indices $i$ and $j$, that is, the number of values in the
consecutive run. Therefore, when a pointer is moving from front to back, it can see the run length for
a given value, and a pointer moving in the opposite direction can see it also.

We also need to maintain the $run\_lengths$ array under the restructuring that takes place during
cracking, which requires that we swap the built runs around in memory, while maintaining the
necessary invariants regarding the position of the two tightening edge pointers used during cracking.
This must also take account of the fact that the number of elements being swapped to an edge
pointer during an iteration may be different to the number of elements being swapped from the edge
pointer back onto the iteration pointer.

\subsubsection{Per-fragment}

A lazier, per-fragment compression policy works as follows: After performing a scan, the cracker
index is updated with the boundary values for any created fragment(s), therefore if two values are
stored which are minimally different, we can then compress the fragment stored between the indices
corresponding to the two boundary values.

\subsection{To compact or not to compact}

After identifying a column fragment containing edges only to/from a single vertex, we have a choice 
in how we proceed in compressing it. We can simply apply the knowledge that a fragment is uniform,
without making any structural changes to the data, or we can actually compact the run-length
encoded fragment, deleting the duplicated elements.

\subsubsection{Cost of compaction}

It is important to note that the columns are implemented as arrays, and as such all reside in
contigious memory, therefore when a range of contigiuos elements are deleted, the elements after
that range must be shifted back towards the front of the array. This memory shift is expensive,
and the cost increases with the amount of data that must be copied over.

It can be said that compaction saves memory, however if it was crucial to save memory, then cracking
would not be an appropriate technique anyway, so we do not consider it an important benefit.

\subsubsection{Without compaction}

By identifying a uniform fragment, we can know that there is no need to read more than one element
of a given run for a cracked column. Therefore we do not need to send the repeated values from
memory over to the application, instead sending the single value and the corresponding rows of the
table. In that case that we have selected multiple compressed vertices, we need to store offsets
into the vector of tuples we return in order to let the application know which tuples correspond to
which compressed value.

The main advantage of this method is that it avoids the deletion overhead incurred by compcating,
which saves time during queries in which a compression opportunity first arises.

\subsection{Exploitation}

\label{ss:compressionexploitation}
