\chapter{Adaptive Compression}

\label{ch:adaptivecompression}

\section{Cracking}

In this chapter we examine ways of performing online compression of the adjacency list representing
the graph by modifying the existing cracking algorithms presented by Ideos' et al.. The background
subsection on cracking covers the terms we will use in this chapter to explain the modifications we
made.

\section{Compression}

In this section we demonstrate that online restructuring of an adjacency list into a CSR formatted 
adjacency matrix is feasible by using cracking, and discuss how this can be exploited to improve the
performance of the execution of graph traversal algorithms.

Our motivation for applying compression is to acheive a speed up by reducing the number of memory
accesses required in the execution of an algorithm. We will demonstrate in section
\ref{ss:compressionexploitation} how we exploit compression.

\subsection{Opportunities}

When applying a run-length encoding to either the $src$ or $dst$ columns, we can gather compression
information with varying eagerness in the course of a cracking scan. We could reserve a decision on
compression until we know that an entire column fragment can be compressed, or we can make many small
compressions in the course of scans of a column, which would require us to store all that
information.

Todo: Make some pictures in that program on the macs in labs

\subsubsection{Per-fragment}

A per-fragment compression policy would be to compress only when entire column fragments are
known to be homogenous. After performing a scan, the cracker index is updated with the boundary
values for any created fragment(s). If two values are stored which are \texttt{minimally different},
then we can compress the column between those indices, which correspond to the boundary values of the
homogenous fragment.

We use the phrase \texttt{minimally different} to describe two values for which there exists no value
between them in the range of values that can be taken for their common type. For example, if we
consider the integers, the values 2 and 3 are minimally different, because there exists no integer
between these values. For a datatype such as a floating point number, this value will vary depending
on the accuracy with which the user wishes to cluster - it may be acceptable to the application that
the values 2.33334 and 2.33279 both be considered 2.33, however, unlike in the integer case, there is
information being lost during compression, which may not be acceptable to the user, which would prompt
them to use a greater granularity for the compression, or to not compress at all.

Compared to the RLE method, this is very easy to implement. One must simply check the boundaries of
newly created fragments and determine if any fragments can now be known to be homogenous.

\subsubsection{Run-length encoding}

In the most eager case, we build and maintain information about all runs of consecutive nodes in the
cracker column of the adjacency list. This is done inside an array, $run\_lengths$.

The cracker column is scanned both front to back and back to front, therefore it would be preferable
that information be stored such that it can be applied in both scanning directions. For a given run of
consecutive values in the cracker column from index $i$ to index $j$ inclusive, the $run\_lengths$
array stores the value $1 + j - i$ at both indices $i$ and $j$, that is, the number of values in the
consecutive run. Therefore, when a pointer is moving from front to back, it can see the run length for
a given value, and a pointer moving in the opposite direction can see it also.

We also need to maintain the $run\_lengths$ array under the restructuring that takes place during
cracking to make it worthwhile, which requires that we be be careful in maintaining all of the
necessary invariants regarding the position of the two tightening edge pointers used during cracking.
This must also take account of the fact that the number of elements being swapped to an edge
pointer during an iteration may be different to the number of elements being swapped from the edge
pointer back onto the iteration pointer. Additionally, when making these swaps of runs, the
$run\_lengths$ array may have need to have its entries modified in order to retain consistency. There
are two potential approaches to swapping around runs of different lengths within the
$run\_lengths$ array.

The first is to swap the entire of the shorter run and modify entries of the $run\_lengths$ array
corresponding to the longer run to maintain consistency after the swap. This means that the longer
run is not fully swapped, so we have called it "underswapping". Underswapping causes the longer run
to get broken up.

The second approach is to swap the entire of the longer run, and pad the shorter run with more
elements. This requires the padded elements to have their runs checked and potentially modified, due
to the possibility that they are constituents of another run. Due to the fact that the shorter run is
swapped with more elements than are actually in that run (by padding), we call this "overswapping".
Overswapping Does not necessarily cause the break-up of runs, however it is more complex.

In this chapter we will detail underswapping and overswapping, along with an evaluation of their
relative performance and a discussion thereof.

\subsubsubsection{Underswapping RLE}

\subsubsubsection{Overswapping RLE}

When overswapping, the entire of the shorter run we call the main section of the swapped range of
values. The padding, we call the remainder.

The idea is that we update entries in $run\_lengths$ such that after doing the swap, the runs will
still be consistent.

\subsection{To compact or not to compact}

After identifying a homogenous section of the column, we have a choice in how we proceed in
compressing it. We can simply apply the knowledge that a fragment is uniform, without making any
structural changes to the data, or we can actually compact the run-length encoded fragment, deleting
the duplicated elements. We will refer to this different modes as compactive, for when we make the
decision to compact, and recignitive, for when we make the decision merely to recognise the
homogenity of the section.

\subsubsection{Recognitive}

After recognising a section of column which is uniform, if we intend to exploit this information, then
we need it stored somewhere.

If we are doing per-fragment compression, then it is sufficient to use the information already stored
in the cracker index, which we used in order to recognise the opportunity in the first place.

Otherwise, if we are applying a run-length encoding across the column, we need a separate structure to
hold the information about each of the runs.

The main advantage of this over compaction is that it avoids the overhead of deleting arbitrary ranges
from an array, thus avoiding unnecessary memcpy calls. However, this comes at the cost of potential
locality improvements and any associated exploitation.

\subsubsection{Cost of compaction}

It is important to note that the columns are implemented as arrays, and as such all reside in
contigious memory, therefore when a range of contigiuos elements are deleted, the elements after
that range must be shifted back towards the front of the array. This memory shift is expensive,
and the cost increases with the amount of data that must be copied over.

It can be said that compaction saves memory, however if it was crucial to save memory, then cracking
would not be an appropriate technique anyway, so we do not consider it an important benefit.

\subsection{Exploitation}

\label{ss:compressionexploitation}
